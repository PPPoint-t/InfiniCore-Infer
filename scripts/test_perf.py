"""
Multi-request and multi-concurrency Tool

Usage:
    python test_perf.py <num_requests> <concurrency> <api_url> <model> [--verbose]

Arguments:
    <num_requests>    Number of total requests to send (e.g. 100)
    <concurrency>     Number of concurrent requests (e.g. 10)
    <api_url>         OpenAI-compatible base URL (e.g. http://localhost:8000)
    <model>           Model name to test (e.g. jiuge)
    --verbose         (Optional) Print per-request details

Example:
    python scripts/test_perf.py 50 5 http://127.0.0.1:8000 jiuge --verbose
"""

import argparse
import asyncio
import time
import random
from openai import AsyncOpenAI

PROMPTS = [
    "å¦‚æžœçŒ«èƒ½å†™è¯—ï¼Œå®ƒä»¬ä¼šå†™äº›ä»€ä¹ˆï¼Ÿ",
    "æè¿°ä¸€ä¸ªæ²¡æœ‰é‡åŠ›çš„ä¸–ç•Œã€‚",
    "å¦‚æžœåœ°çƒåœæ­¢è‡ªè½¬ï¼Œä¼šå‘ç”Ÿä»€ä¹ˆï¼Ÿ",
    "å‡è®¾ä½ æ˜¯ä¸€åªä¼šé£žçš„é²¸é±¼ï¼Œæè¿°ä½ çš„æ—¥å¸¸ç”Ÿæ´»ã€‚",
    "å¦‚æžœäººç±»å¯ä»¥ä¸Žæ¤ç‰©æ²Ÿé€šï¼Œä¸–ç•Œä¼šå˜æˆä»€ä¹ˆæ ·ï¼Ÿ",
    "æè¿°ä¸€ä¸ªç”±ç³–æžœæž„æˆçš„åŸŽå¸‚ã€‚",
    "å¦‚æžœæ—¶é—´æ—…è¡Œæˆä¸ºå¯èƒ½ï¼Œä½ æœ€æƒ³åŽ»å“ªä¸ªæ—¶ä»£ï¼Ÿ",
    "æƒ³è±¡ä¸€ä¸‹ï¼Œå¦‚æžœåœ°çƒä¸Šåªæœ‰è“è‰²ï¼Œå…¶ä»–é¢œè‰²éƒ½æ¶ˆå¤±äº†ã€‚",
    "å¦‚æžœåŠ¨ç‰©èƒ½ä¸Šç½‘ï¼Œå®ƒä»¬ä¼šæµè§ˆä»€ä¹ˆç½‘ç«™ï¼Ÿ",
    "æè¿°ä¸€ä¸ªæ²¡æœ‰å£°éŸ³çš„ä¸–ç•Œã€‚",
    "å¦‚æžœäººç±»å¯ä»¥åœ¨æ°´ä¸‹å‘¼å¸ï¼ŒåŸŽå¸‚ä¼šå¦‚ä½•å˜åŒ–ï¼Ÿ",
    "æƒ³è±¡ä¸€ä¸‹ï¼Œå¦‚æžœå¤©ç©ºæ˜¯ç»¿è‰²çš„ï¼Œäº‘æ˜¯ç´«è‰²çš„ã€‚",
    "å¦‚æžœä½ èƒ½ä¸Žä»»ä½•åŽ†å²äººç‰©å…±è¿›æ™šé¤ï¼Œä½ ä¼šé€‰æ‹©è°ï¼Ÿ",
    "æè¿°ä¸€ä¸ªæ²¡æœ‰å¤œæ™šçš„æ˜Ÿçƒã€‚",
    "å¦‚æžœåœ°çƒä¸Šåªæœ‰ä¸€ç§è¯­è¨€ï¼Œä¸–ç•Œä¼šå¦‚ä½•è¿ä½œï¼Ÿ",
    "æƒ³è±¡ä¸€ä¸‹ï¼Œå¦‚æžœæ‰€æœ‰çš„ä¹¦éƒ½å˜æˆäº†éŸ³ä¹ã€‚",
    "å¦‚æžœä½ å¯ä»¥å˜æˆä»»ä½•ä¸€ç§åŠ¨ç‰©ï¼Œä½ ä¼šé€‰æ‹©ä»€ä¹ˆï¼Ÿ",
    "æè¿°ä¸€ä¸ªç”±æœºå™¨äººç»Ÿæ²»çš„æœªæ¥ä¸–ç•Œã€‚",
    "å¦‚æžœä½ èƒ½ä¸Žä»»ä½•è™šæž„è§’è‰²æˆä¸ºæœ‹å‹ï¼Œä½ ä¼šé€‰æ‹©è°ï¼Ÿ",
    "æƒ³è±¡ä¸€ä¸‹ï¼Œå¦‚æžœæ¯ä¸ªäººéƒ½èƒ½è¯»æ‡‚ä»–äººçš„æ€æƒ³ã€‚"
]

def ljust_cn(s: str, width: int) -> str:
    """Pad a string to width accounting for mixed char widths"""
    # crude approximation: Chinese chars count as width 2
    pad = width - sum(2 if ord(c) > 255 else 1 for c in s)
    return s + " " * max(0, pad)

async def benchmark_user(worker_id, client, semaphore, queue, results, model, verbose):
    while True:
        async with semaphore:
            task_id = await queue.get()
            if task_id is None:
                queue.task_done()
                break
            start = time.time()
            prompt = random.choice(PROMPTS)
            if verbose:
                print(f"ðŸš€ [Req {task_id}] Worker-{worker_id} Prompt: {prompt}")
            try:
                stream = await client.chat.completions.create(
                    model=model,
                    messages=[{"role": "user", "content": prompt}],
                    stream=True
                )
                first_time = None
                tokens = 0
                response = ""
                async for chunk in stream:
                    txt = chunk.choices[0].delta.content
                    if txt:
                        response += txt
                        tokens += 1
                        if first_time is None:
                            first_time = time.time()
                    if chunk.choices[0].finish_reason is not None:
                        break
                end = time.time()
                latency = end - start
                ttft = (first_time - start) if first_time else 0
                avg_tok = latency / tokens * 1000 if tokens else 0
                results.append((task_id, worker_id, ttft, latency, tokens, avg_tok, prompt, response))
                if verbose:
                    print("""
   TTFT            : {:.3f} s
   Latency         : {:.3f} s
   Tokens         : {}
   Avg/token      : {:.2f} ms
   Response       : {}...
""".format(ttft, latency, tokens, avg_tok, response[:100]))
            except Exception as e:
                print(f"âŒ [Req {task_id}] Worker-{worker_id} failed: {e}")
            finally:
                queue.task_done()

async def run_benchmark(num_requests, concurrency, api_url, model, verbose):
    print(f"ðŸ”— Connecting to {api_url}, model {model}")
    client = AsyncOpenAI(base_url=api_url, api_key="default")
    sem = asyncio.Semaphore(concurrency)
    queue = asyncio.Queue()
    results = []
    for i in range(num_requests): await queue.put(i)
    for _ in range(concurrency): await queue.put(None)
    tasks = [asyncio.create_task(benchmark_user(i, client, sem, queue, results, model, verbose)) for i in range(concurrency)]
    st = time.time()
    await queue.join()
    await asyncio.gather(*tasks)
    et = time.time()
    total = et - st
    success = len(results)
    rps = success / total if total else 0
    total_tokens = sum(r[4] for r in results)
    avg_latency = sum(r[3] for r in results)/success if success else 0
    avg_ttft = sum(r[2] for r in results)/success if success else 0
    avg_tok_ms = sum(r[5] for r in results)/success if success else 0
    tok_speed = total_tokens/total if total else 0
    w = 26
    print("\n=== ðŸ“Š æ€§èƒ½æŒ‡æ ‡æ±‡æ€» ===")
    print("-"*60)
    print(f"{ljust_cn('å¹¶å‘æ•°',w)}: {concurrency}")
    print(f"{ljust_cn('è¯·æ±‚æ€»æ•°',w)}: {num_requests}")
    print(f"{ljust_cn('æˆåŠŸè¯·æ±‚æ•°',w)}: {success}")
    print(f"{ljust_cn('æ€»è€—æ—¶',w)}: {total:>7.2f} s")
    print(f"{ljust_cn('æ€»è¾“å‡ºtokenæ•°',w)}: {total_tokens}")
    print(f"{ljust_cn('è¯·æ±‚é€ŸçŽ‡ (RPS)',w)}: {rps:>7.2f} req/s")
    print(f"{ljust_cn('Average latency',w)}: {avg_latency:>7.2f} s")
    print(f"{ljust_cn('Average TTFT',w)}: {avg_ttft:>7.2f} s")
    print(f"{ljust_cn('Avg time per token',w)}: {avg_tok_ms:>7.2f} ms/token")
    print(f"{ljust_cn('Token generation speed',w)}: {tok_speed:>7.2f} tok/s")
    print("-"*60)

if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='Multi-concurrency performance testing')
    parser.add_argument('num_requests', type=int, help='total requests')
    parser.add_argument('concurrency', type=int, help='parallel workers')
    parser.add_argument('api_url', type=str, help='API base URL')
    parser.add_argument('model', type=str, help='model name')
    parser.add_argument('--verbose', action='store_true', help='detailed per-request info')
    args = parser.parse_args()
    asyncio.run(run_benchmark(args.num_requests, args.concurrency, args.api_url, args.model, args.verbose))
